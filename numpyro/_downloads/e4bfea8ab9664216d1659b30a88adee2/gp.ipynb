{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nGaussian Process\n================\n\nIn this example we show how to use NUTS to sample from the posterior\nover the hyperparameters of a gaussian process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import argparse\nimport os\nimport time\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport jax\nfrom jax import vmap\nimport jax.numpy as jnp\nimport jax.random as random\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS\n\nmatplotlib.use('Agg')  # noqa: E402\n\n\n# squared exponential kernel with diagonal noise term\ndef kernel(X, Z, var, length, noise, jitter=1.0e-6, include_noise=True):\n    deltaXsq = jnp.power((X[:, None] - Z) / length, 2.0)\n    k = var * jnp.exp(-0.5 * deltaXsq)\n    if include_noise:\n        k += (noise + jitter) * jnp.eye(X.shape[0])\n    return k\n\n\ndef model(X, Y):\n    # set uninformative log-normal priors on our three kernel hyperparameters\n    var = numpyro.sample(\"kernel_var\", dist.LogNormal(0.0, 10.0))\n    noise = numpyro.sample(\"kernel_noise\", dist.LogNormal(0.0, 10.0))\n    length = numpyro.sample(\"kernel_length\", dist.LogNormal(0.0, 10.0))\n\n    # compute kernel\n    k = kernel(X, X, var, length, noise)\n\n    # sample Y according to the standard gaussian process formula\n    numpyro.sample(\"Y\", dist.MultivariateNormal(loc=jnp.zeros(X.shape[0]), covariance_matrix=k),\n                   obs=Y)\n\n\n# helper function for doing hmc inference\ndef run_inference(model, args, rng_key, X, Y):\n    start = time.time()\n    kernel = NUTS(model)\n    mcmc = MCMC(kernel, args.num_warmup, args.num_samples, num_chains=args.num_chains,\n                progress_bar=False if \"NUMPYRO_SPHINXBUILD\" in os.environ else True)\n    mcmc.run(rng_key, X, Y)\n    mcmc.print_summary()\n    print('\\nMCMC elapsed time:', time.time() - start)\n    return mcmc.get_samples()\n\n\n# do GP prediction for a given set of hyperparameters. this makes use of the well-known\n# formula for gaussian process predictions\ndef predict(rng_key, X, Y, X_test, var, length, noise):\n    # compute kernels between train and test data, etc.\n    k_pp = kernel(X_test, X_test, var, length, noise, include_noise=True)\n    k_pX = kernel(X_test, X, var, length, noise, include_noise=False)\n    k_XX = kernel(X, X, var, length, noise, include_noise=True)\n    K_xx_inv = jnp.linalg.inv(k_XX)\n    K = k_pp - jnp.matmul(k_pX, jnp.matmul(K_xx_inv, jnp.transpose(k_pX)))\n    sigma_noise = jnp.sqrt(jnp.clip(jnp.diag(K), a_min=0.)) * jax.random.normal(rng_key, X_test.shape[:1])\n    mean = jnp.matmul(k_pX, jnp.matmul(K_xx_inv, Y))\n    # we return both the mean function and a sample from the posterior predictive for the\n    # given set of hyperparameters\n    return mean, mean + sigma_noise\n\n\n# create artificial regression dataset\ndef get_data(N=30, sigma_obs=0.15, N_test=400):\n    np.random.seed(0)\n    X = jnp.linspace(-1, 1, N)\n    Y = X + 0.2 * jnp.power(X, 3.0) + 0.5 * jnp.power(0.5 + X, 2.0) * jnp.sin(4.0 * X)\n    Y += sigma_obs * np.random.randn(N)\n    Y -= jnp.mean(Y)\n    Y /= jnp.std(Y)\n\n    assert X.shape == (N,)\n    assert Y.shape == (N,)\n\n    X_test = jnp.linspace(-1.3, 1.3, N_test)\n\n    return X, Y, X_test\n\n\ndef main(args):\n    X, Y, X_test = get_data(N=args.num_data)\n\n    # do inference\n    rng_key, rng_key_predict = random.split(random.PRNGKey(0))\n    samples = run_inference(model, args, rng_key, X, Y)\n\n    # do prediction\n    vmap_args = (random.split(rng_key_predict, args.num_samples * args.num_chains), samples['kernel_var'],\n                 samples['kernel_length'], samples['kernel_noise'])\n    means, predictions = vmap(lambda rng_key, var, length, noise:\n                              predict(rng_key, X, Y, X_test, var, length, noise))(*vmap_args)\n\n    mean_prediction = np.mean(means, axis=0)\n    percentiles = np.percentile(predictions, [5.0, 95.0], axis=0)\n\n    # make plots\n    fig, ax = plt.subplots(1, 1)\n\n    # plot training data\n    ax.plot(X, Y, 'kx')\n    # plot 90% confidence level of predictions\n    ax.fill_between(X_test, percentiles[0, :], percentiles[1, :], color='lightblue')\n    # plot mean prediction\n    ax.plot(X_test, mean_prediction, 'blue', ls='solid', lw=2.0)\n    ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"Mean predictions with 90% CI\")\n\n    plt.savefig(\"gp_plot.pdf\")\n    plt.tight_layout()\n\n\nif __name__ == \"__main__\":\n    assert numpyro.__version__.startswith('0.3.0')\n    parser = argparse.ArgumentParser(description=\"Gaussian Process example\")\n    parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=1000, type=int)\n    parser.add_argument(\"--num-warmup\", nargs='?', default=1000, type=int)\n    parser.add_argument(\"--num-chains\", nargs='?', default=1, type=int)\n    parser.add_argument(\"--num-data\", nargs='?', default=25, type=int)\n    parser.add_argument(\"--device\", default='cpu', type=str, help='use \"cpu\" or \"gpu\".')\n    args = parser.parse_args()\n\n    numpyro.set_platform(args.device)\n    numpyro.set_host_device_count(args.num_chains)\n\n    main(args)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}