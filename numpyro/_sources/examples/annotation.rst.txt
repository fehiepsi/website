.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_examples_annotation.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_examples_annotation.py:


Bayesian Models of Annotation
=============================

In this example, we run MCMC for various crowdsourced annotation models in [1].

All models have discrete latent variables. Under the hood, we enumerate over
(marginalize out) those discrete latent sites in inference. Those models have different
complexity so they are great refererences for those who are new to Pyro/NumPyro
enumeration mechanism. We recommend readers compare the implementations with the
corresponding plate diagrams in [1] to see how concise a Pyro/NumPyro program is.

The interested readers can also refer to [3] for more explanation about enumeration.

The data is taken from Table 1 of reference [2].

Currently, this example does not include postprocessing steps to deal with "Label
Switching" issue (mentioned in section 6.2 of [1]).

**References:**

    1. Paun, S., Carpenter, B., Chamberlain, J., Hovy, D., Kruschwitz, U.,
       and Poesio, M. (2018). "Comparing bayesian models of annotation"
       (https://www.aclweb.org/anthology/Q18-1040/)
    2. Dawid, A. P., and Skene, A. M. (1979).
       "Maximum likelihood estimation of observer error‚Äêrates using the EM algorithm"
    3. "Inference with Discrete Latent Variables"
       (http://pyro.ai/examples/enumeration.html)




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


                     mean       std    median      5.0%     95.0%     n_eff     r_hat
    beta[0,0,0]      0.84      0.05      0.85      0.77      0.92   3412.27      1.00
    beta[0,0,1]      0.12      0.04      0.12      0.05      0.19   2479.40      1.00
    beta[0,0,2]      0.02      0.02      0.01      0.00      0.04   1304.53      1.00
    beta[0,0,3]      0.02      0.02      0.01      0.00      0.04   1275.94      1.00
    beta[0,1,0]      0.10      0.12      0.07      0.00      0.25    216.18      1.02
    beta[0,1,1]      0.29      0.32      0.12      0.00      0.85      5.48      1.29
    beta[0,1,2]      0.35      0.30      0.24      0.00      0.81     29.50      1.06
    beta[0,1,3]      0.25      0.24      0.18      0.00      0.63     18.07      1.09
    beta[0,2,0]      0.08      0.08      0.06      0.00      0.14     58.56      1.05
    beta[0,2,1]      0.70      0.29      0.83      0.10      0.94      5.06      1.29
    beta[0,2,2]      0.13      0.16      0.08      0.00      0.22     14.25      1.10
    beta[0,2,3]      0.09      0.19      0.02      0.00      0.43      7.14      1.19
    beta[0,3,0]      0.09      0.10      0.05      0.00      0.20    246.70      1.00
    beta[0,3,1]      0.10      0.11      0.07      0.00      0.25    371.16      1.00
    beta[0,3,2]      0.58      0.27      0.67      0.10      0.92     77.30      1.00
    beta[0,3,3]      0.23      0.21      0.15      0.00      0.57     91.12      1.00
    beta[1,0,0]      0.71      0.10      0.72      0.55      0.87   1731.84      1.00
    beta[1,0,1]      0.20      0.09      0.19      0.07      0.35   1751.49      1.00
    beta[1,0,2]      0.05      0.05      0.03      0.00      0.10    969.04      1.00
    beta[1,0,3]      0.05      0.04      0.03      0.00      0.11   1143.78      1.00
    beta[1,1,0]      0.15      0.15      0.10      0.00      0.36    213.65      1.01
    beta[1,1,1]      0.24      0.20      0.18      0.00      0.53      9.36      1.14
    beta[1,1,2]      0.32      0.19      0.31      0.00      0.58    164.84      1.01
    beta[1,1,3]      0.29      0.23      0.27      0.00      0.63     13.62      1.10
    beta[1,2,0]      0.10      0.09      0.08      0.00      0.20     33.26      1.05
    beta[1,2,1]      0.45      0.17      0.48      0.12      0.70      9.07      1.14
    beta[1,2,2]      0.33      0.13      0.34      0.12      0.57    104.77      1.03
    beta[1,2,3]      0.11      0.17      0.04      0.00      0.36      7.60      1.18
    beta[1,3,0]      0.14      0.14      0.10      0.00      0.33    548.48      1.00
    beta[1,3,1]      0.14      0.13      0.10      0.00      0.33    703.48      1.00
    beta[1,3,2]      0.37      0.20      0.36      0.01      0.64    156.43      1.00
    beta[1,3,3]      0.35      0.19      0.33      0.04      0.63    488.55      1.00
    beta[2,0,0]      0.87      0.07      0.88      0.76      0.97   1722.84      1.00
    beta[2,0,1]      0.04      0.04      0.03      0.00      0.10   1258.76      1.00
    beta[2,0,2]      0.04      0.04      0.03      0.00      0.09   1164.26      1.00
    beta[2,0,3]      0.05      0.05      0.03      0.00      0.11   1313.44      1.00
    beta[2,1,0]      0.15      0.14      0.11      0.00      0.34    299.26      1.01
    beta[2,1,1]      0.32      0.25      0.23      0.00      0.72      6.88      1.20
    beta[2,1,2]      0.24      0.16      0.21      0.00      0.47    104.40      1.01
    beta[2,1,3]      0.28      0.21      0.26      0.00      0.58     12.70      1.12
    beta[2,2,0]      0.12      0.10      0.09      0.00      0.23    132.88      1.03
    beta[2,2,1]      0.59      0.24      0.67      0.08      0.84      5.87      1.24
    beta[2,2,2]      0.19      0.12      0.16      0.01      0.33     38.80      1.05
    beta[2,2,3]      0.11      0.17      0.04      0.00      0.36      8.00      1.17
    beta[2,3,0]      0.14      0.13      0.10      0.00      0.33    513.00      1.00
    beta[2,3,1]      0.22      0.15      0.20      0.00      0.43    816.16      1.00
    beta[2,3,2]      0.28      0.16      0.27      0.02      0.51    918.86      1.00
    beta[2,3,3]      0.35      0.17      0.34      0.05      0.59   1206.85      1.00
    beta[3,0,0]      0.80      0.08      0.81      0.66      0.93   2865.72      1.00
    beta[3,0,1]      0.11      0.07      0.10      0.02      0.21   1979.19      1.00
    beta[3,0,2]      0.05      0.04      0.03      0.00      0.10   1087.06      1.00
    beta[3,0,3]      0.04      0.04      0.03      0.00      0.10   1267.23      1.00
    beta[3,1,0]      0.14      0.13      0.11      0.00      0.32    185.57      1.02
    beta[3,1,1]      0.28      0.25      0.17      0.00      0.70      6.47      1.21
    beta[3,1,2]      0.30      0.22      0.25      0.00      0.63     76.90      1.02
    beta[3,1,3]      0.28      0.21      0.22      0.00      0.59     18.13      1.08
    beta[3,2,0]      0.10      0.09      0.08      0.01      0.21     39.74      1.05
    beta[3,2,1]      0.57      0.22      0.65      0.11      0.83      6.33      1.22
    beta[3,2,2]      0.19      0.13      0.16      0.02      0.34     45.52      1.04
    beta[3,2,3]      0.13      0.14      0.09      0.00      0.32      9.98      1.14
    beta[3,3,0]      0.14      0.13      0.10      0.00      0.33    617.12      1.00
    beta[3,3,1]      0.14      0.13      0.10      0.00      0.32    397.99      1.00
    beta[3,3,2]      0.45      0.23      0.47      0.05      0.79    118.40      1.00
    beta[3,3,3]      0.27      0.19      0.24      0.00      0.55    236.56      1.00
    beta[4,0,0]      0.84      0.07      0.86      0.73      0.96   1840.35      1.00
    beta[4,0,1]      0.07      0.05      0.05      0.00      0.14   1189.80      1.00
    beta[4,0,2]      0.05      0.04      0.03      0.00      0.11   1093.41      1.00
    beta[4,0,3]      0.04      0.04      0.03      0.00      0.10   1110.44      1.00
    beta[4,1,0]      0.16      0.14      0.13      0.00      0.36    664.83      1.00
    beta[4,1,1]      0.29      0.22      0.23      0.00      0.63      9.33      1.16
    beta[4,1,2]      0.29      0.18      0.26      0.01      0.57    126.42      1.01
    beta[4,1,3]      0.25      0.20      0.22      0.00      0.54     15.50      1.11
    beta[4,2,0]      0.17      0.11      0.16      0.03      0.30    653.93      1.01
    beta[4,2,1]      0.51      0.20      0.56      0.09      0.76      7.23      1.20
    beta[4,2,2]      0.22      0.13      0.20      0.01      0.37     92.19      1.02
    beta[4,2,3]      0.10      0.15      0.04      0.00      0.30      8.56      1.16
    beta[4,3,0]      0.14      0.12      0.10      0.00      0.30    804.36      1.00
    beta[4,3,1]      0.22      0.16      0.19      0.00      0.45   1638.38      1.00
    beta[4,3,2]      0.37      0.19      0.37      0.01      0.62    238.16      1.00
    beta[4,3,3]      0.28      0.18      0.24      0.00      0.53    483.54      1.00
          pi[0]      0.40      0.07      0.40      0.28      0.51   1624.13      1.00
          pi[1]      0.15      0.15      0.09      0.00      0.43      5.13      1.30
          pi[2]      0.35      0.17      0.41      0.01      0.52      5.44      1.26
          pi[3]      0.10      0.06      0.09      0.00      0.17    119.86      1.00

    Number of divergences: 0






|


.. code-block:: default


    import argparse
    import os

    import numpy as np

    from jax import nn, random
    import jax.numpy as jnp

    import numpyro
    from numpyro import handlers
    from numpyro.contrib.indexing import Vindex
    import numpyro.distributions as dist
    from numpyro.infer import MCMC, NUTS
    from numpyro.infer.reparam import LocScaleReparam


    def get_data():
        """
        :return: a tuple of annotator indices and class indices. The first term has shape
            `num_positions` whose entries take values from `0` to `num_annotators - 1`.
            The second term has shape `num_items x num_positions` whose entries take values
            from `0` to `num_classes - 1`.
        """
        # NB: the first annotator assessed each item 3 times
        positions = np.array([1, 1, 1, 2, 3, 4, 5])
        annotations = np.array([
            [1, 3, 1, 2, 2, 2, 1, 3, 2, 2, 4, 2, 1, 2, 1,
             1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1,
             1, 3, 1, 2, 2, 4, 2, 2, 3, 1, 1, 1, 2, 1, 2],
            [1, 3, 1, 2, 2, 2, 2, 3, 2, 3, 4, 2, 1, 2, 2,
             1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 3, 1, 1, 1,
             1, 3, 1, 2, 2, 3, 2, 3, 3, 1, 1, 2, 3, 2, 2],
            [1, 3, 2, 2, 2, 2, 2, 3, 2, 2, 4, 2, 1, 2, 1,
             1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2,
             1, 3, 1, 2, 2, 3, 1, 2, 3, 1, 1, 1, 2, 1, 2],
            [1, 4, 2, 3, 3, 3, 2, 3, 2, 2, 4, 3, 1, 3, 1,
             2, 1, 1, 2, 1, 2, 2, 3, 2, 1, 1, 2, 1, 1, 1,
             1, 3, 1, 2, 3, 4, 2, 3, 3, 1, 1, 2, 2, 1, 2],
            [1, 3, 1, 1, 2, 3, 1, 4, 2, 2, 4, 3, 1, 2, 1,
             1, 1, 1, 2, 3, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1,
             1, 2, 1, 2, 2, 3, 2, 2, 4, 1, 1, 1, 2, 1, 2],
            [1, 3, 2, 2, 2, 2, 1, 3, 2, 2, 4, 4, 1, 1, 1,
             1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2,
             1, 3, 1, 2, 3, 4, 3, 3, 3, 1, 1, 1, 2, 1, 2],
            [1, 4, 2, 1, 2, 2, 1, 3, 3, 3, 4, 3, 1, 2, 1,
             1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1,
             1, 3, 1, 2, 2, 3, 2, 3, 2, 1, 1, 1, 2, 1, 2],
        ]).T
        # we minus 1 because in Python, the first index is 0
        return positions - 1, annotations - 1


    def multinomial(annotations):
        """
        This model corresponds to the plate diagram in Figure 1 of reference [1].
        """
        num_classes = int(np.max(annotations)) + 1
        num_items, num_positions = annotations.shape

        with numpyro.plate("class", num_classes):
            zeta = numpyro.sample("zeta", dist.Dirichlet(jnp.ones(num_classes)))

        pi = numpyro.sample("pi", dist.Dirichlet(jnp.ones(num_classes)))

        with numpyro.plate("item", num_items, dim=-2):
            c = numpyro.sample("c", dist.Categorical(pi))

            with numpyro.plate("position", num_positions):
                numpyro.sample("y", dist.Categorical(zeta[c]), obs=annotations)


    def dawid_skene(positions, annotations):
        """
        This model corresponds to the plate diagram in Figure 2 of reference [1].
        """
        num_annotators = int(np.max(positions)) + 1
        num_classes = int(np.max(annotations)) + 1
        num_items, num_positions = annotations.shape

        with numpyro.plate("annotator", num_annotators, dim=-2):
            with numpyro.plate("class", num_classes):
                beta = numpyro.sample("beta", dist.Dirichlet(jnp.ones(num_classes)))

        pi = numpyro.sample("pi", dist.Dirichlet(jnp.ones(num_classes)))

        with numpyro.plate("item", num_items, dim=-2):
            c = numpyro.sample("c", dist.Categorical(pi))

            # here we use Vindex to allow broadcasting for the second index `c`
            # ref: http://num.pyro.ai/en/latest/utilities.html#numpyro.contrib.indexing.vindex
            with numpyro.plate("position", num_positions):
                numpyro.sample("y", dist.Categorical(Vindex(beta)[positions, c, :]), obs=annotations)


    def mace(positions, annotations):
        """
        This model corresponds to the plate diagram in Figure 3 of reference [1].
        """
        num_annotators = int(np.max(positions)) + 1
        num_classes = int(np.max(annotations)) + 1
        num_items, num_positions = annotations.shape

        with numpyro.plate("annotator", num_annotators):
            epsilon = numpyro.sample("epsilon", dist.Dirichlet(jnp.full(num_classes, 10)))
            theta = numpyro.sample("theta", dist.Beta(0.5, 0.5))

        with numpyro.plate("item", num_items, dim=-2):
            # NB: using constant logits for discrete uniform prior
            # (NumPyro does not have DiscreteUniform distribution yet)
            c = numpyro.sample("c", dist.Categorical(logits=jnp.zeros(num_classes)))

            with numpyro.plate("position", num_positions):
                s = numpyro.sample("s", dist.Bernoulli(1 - theta[positions]))
                probs = jnp.where(s[..., None] == 0, nn.one_hot(c, num_classes), epsilon[positions])
                numpyro.sample("y", dist.Categorical(probs), obs=annotations)


    def hierarchical_dawid_skene(positions, annotations):
        """
        This model corresponds to the plate diagram in Figure 4 of reference [1].
        """
        num_annotators = int(np.max(positions)) + 1
        num_classes = int(np.max(annotations)) + 1
        num_items, num_positions = annotations.shape

        with numpyro.plate("class", num_classes):
            # NB: we define `beta` as the `logits` of `y` likelihood; but `logits` is
            # invariant up to a constant, so we'll follow [1]: fix the last term of `beta`
            # to 0 and only define hyperpriors for the first `num_classes - 1` terms.
            zeta = numpyro.sample("zeta", dist.Normal(0, 1).expand([num_classes - 1]).to_event(1))
            omega = numpyro.sample("Omega", dist.HalfNormal(1).expand([num_classes - 1]).to_event(1))

        with numpyro.plate("annotator", num_annotators, dim=-2):
            with numpyro.plate("class", num_classes):
                # non-centered parameterization
                with handlers.reparam(config={"beta": LocScaleReparam(0)}):
                    beta = numpyro.sample("beta", dist.Normal(zeta, omega).to_event(1))
                # pad 0 to the last item
                beta = jnp.pad(beta, [(0, 0)] * (jnp.ndim(beta) - 1) + [(0, 1)])

        pi = numpyro.sample("pi", dist.Dirichlet(jnp.ones(num_classes)))

        with numpyro.plate("item", num_items, dim=-2):
            c = numpyro.sample("c", dist.Categorical(pi))

            with numpyro.plate("position", num_positions):
                logits = Vindex(beta)[positions, c, :]
                numpyro.sample("y", dist.Categorical(logits=logits), obs=annotations)


    def item_difficulty(annotations):
        """
        This model corresponds to the plate diagram in Figure 5 of reference [1].
        """
        num_classes = int(np.max(annotations)) + 1
        num_items, num_positions = annotations.shape

        with numpyro.plate("class", num_classes):
            eta = numpyro.sample("eta", dist.Normal(0, 1).expand([num_classes - 1]).to_event(1))
            chi = numpyro.sample("Chi", dist.HalfNormal(1).expand([num_classes - 1]).to_event(1))

        pi = numpyro.sample("pi", dist.Dirichlet(jnp.ones(num_classes)))

        with numpyro.plate("item", num_items, dim=-2):
            c = numpyro.sample("c", dist.Categorical(pi))

            with handlers.reparam(config={"theta": LocScaleReparam(0)}):
                theta = numpyro.sample("theta", dist.Normal(eta[c], chi[c]).to_event(1))
                theta = jnp.pad(theta, [(0, 0)] * (jnp.ndim(theta) - 1) + [(0, 1)])

            with numpyro.plate("position", annotations.shape[-1]):
                numpyro.sample("y", dist.Categorical(logits=theta), obs=annotations)


    def logistic_random_effects(positions, annotations):
        """
        This model corresponds to the plate diagram in Figure 5 of reference [1].
        """
        num_annotators = int(np.max(positions)) + 1
        num_classes = int(np.max(annotations)) + 1
        num_items, num_positions = annotations.shape

        with numpyro.plate("class", num_classes):
            zeta = numpyro.sample("zeta", dist.Normal(0, 1).expand([num_classes - 1]).to_event(1))
            omega = numpyro.sample("Omega", dist.HalfNormal(1).expand([num_classes - 1]).to_event(1))
            chi = numpyro.sample("Chi", dist.HalfNormal(1).expand([num_classes - 1]).to_event(1))

        with numpyro.plate("annotator", num_annotators, dim=-2):
            with numpyro.plate("class", num_classes):
                with handlers.reparam(config={"beta": LocScaleReparam(0)}):
                    beta = numpyro.sample("beta", dist.Normal(zeta, omega).to_event(1))
                    beta = jnp.pad(beta, [(0, 0)] * (jnp.ndim(beta) - 1) + [(0, 1)])

        pi = numpyro.sample("pi", dist.Dirichlet(jnp.ones(num_classes)))

        with numpyro.plate("item", num_items, dim=-2):
            c = numpyro.sample("c", dist.Categorical(pi))

            with handlers.reparam(config={"theta": LocScaleReparam(0)}):
                theta = numpyro.sample("theta", dist.Normal(0, chi[c]).to_event(1))
                theta = jnp.pad(theta, [(0, 0)] * (jnp.ndim(theta) - 1) + [(0, 1)])

            with numpyro.plate("position", num_positions):
                logits = Vindex(beta)[positions, c, :] - theta
                numpyro.sample("y", dist.Categorical(logits=logits), obs=annotations)


    NAME_TO_MODEL = {
        "mn": multinomial,
        "ds": dawid_skene,
        "mace": mace,
        "hds": hierarchical_dawid_skene,
        "id": item_difficulty,
        "lre": logistic_random_effects,
    }


    def main(args):
        annotators, annotations = get_data()
        model = NAME_TO_MODEL[args.model]
        data = (annotations,) if model in [multinomial, item_difficulty] else (annotators, annotations)

        mcmc = MCMC(
            NUTS(model),
            args.num_warmup,
            args.num_samples,
            num_chains=args.num_chains,
            progress_bar=False if "NUMPYRO_SPHINXBUILD" in os.environ else True,
        )
        mcmc.run(random.PRNGKey(0), *data)
        mcmc.print_summary()


    if __name__ == "__main__":
        assert numpyro.__version__.startswith("0.4.0")
        parser = argparse.ArgumentParser(description="Bayesian Models of Annotation")
        parser.add_argument("-n", "--num-samples", nargs="?", default=1000, type=int)
        parser.add_argument("--num-warmup", nargs="?", default=1000, type=int)
        parser.add_argument("--num-chains", nargs="?", default=1, type=int)
        parser.add_argument(
            "--model",
            nargs="?",
            default="ds",
            help='one of "mn" (multinomial), "ds" (dawid_skene), "mace",'
            ' "hds" (hierarchical_dawid_skene),'
            ' "id" (item_difficulty), "lre" (logistic_random_effects)',
        )
        parser.add_argument("--device", default="cpu", type=str, help='use "cpu" or "gpu".')
        args = parser.parse_args()

        numpyro.set_platform(args.device)
        numpyro.set_host_device_count(args.num_chains)

        main(args)


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  13.034 seconds)


.. _sphx_glr_download_examples_annotation.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: annotation.py <annotation.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: annotation.ipynb <annotation.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
