.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_examples_sparse_regression.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_examples_sparse_regression.py:


Sparse Regression
=================

We demonstrate how to do (fully Bayesian) sparse linear regression using the
approach described in [1]. This approach is particularly suitable for situations
with many feature dimensions (large P) but not too many datapoints (small N).
In particular we consider a quadratic regressor of the form:

.. math::

    f(X) = \text{constant} + \sum_i \theta_i X_i + \sum_{i<j} \theta_{ij} X_i X_j + \text{observation noise}

**References:**

    1. Raj Agrawal, Jonathan H. Huggins, Brian Trippe, Tamara Broderick (2019),
       "The Kernel Interaction Trick: Fast Bayesian Discovery of Pairwise Interactions in High Dimensions",
       (https://arxiv.org/abs/1905.06501)




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


                    mean       std    median      5.0%     95.0%     n_eff     r_hat
          eta1      0.00      0.00      0.00      0.00      0.00    512.18      1.00
     lambda[0]   2930.15  14887.17    770.12     56.75   3634.32    279.33      1.00
     lambda[1]   3464.71  11878.28   1228.01    111.53   6164.95    826.30      1.00
     lambda[2]    272.21    303.10    193.15     21.60    518.53    648.72      1.00
     lambda[3]      1.14      1.52      0.68      0.00      2.59    929.04      1.00
     lambda[4]      1.33      1.63      0.82      0.00      3.10    912.30      1.00
     lambda[5]      1.18      1.58      0.69      0.00      2.62   1105.09      1.00
     lambda[6]      1.66      2.33      0.93      0.00      3.63    634.95      1.00
     lambda[7]      1.50      2.08      0.89      0.01      3.28    818.52      1.00
     lambda[8]      1.02      1.36      0.60      0.00      2.37   1049.26      1.00
     lambda[9]      1.32      1.76      0.81      0.00      2.98    771.58      1.00
    lambda[10]      1.37      2.00      0.85      0.00      2.90   1008.09      1.00
    lambda[11]      2.28      2.78      1.42      0.00      5.28   1022.76      1.00
    lambda[12]      1.35      1.86      0.84      0.00      3.12   1212.69      1.00
    lambda[13]      1.06      1.43      0.65      0.00      2.50   1005.09      1.00
    lambda[14]      1.06      1.36      0.68      0.00      2.31    955.76      1.00
    lambda[15]      1.69      2.77      0.79      0.00      4.09    836.02      1.00
    lambda[16]      3.06      4.38      1.80      0.00      6.68    997.15      1.00
    lambda[17]      1.16      1.37      0.73      0.00      2.61   1054.64      1.00
    lambda[18]      1.16      2.01      0.64      0.00      2.66    719.28      1.00
    lambda[19]      1.33      1.64      0.88      0.00      2.95    806.33      1.00
           msq      0.84      0.68      0.69      0.22      1.46    573.55      1.00
         sigma      0.02      0.00      0.02      0.02      0.02    869.72      1.00
          xisq      0.39      0.27      0.32      0.09      0.70    583.08      1.00

    Number of divergences: 0

    MCMC elapsed time: 25.204463243484497
    Coefficients theta_1 to theta_3 used to generate the data: [0.69618857 0.7082517  0.35156995]
    The single quadratic coefficient theta_{1,2} used to generate the data: 0.4637312
    [dimension 01/20]  active:      6.88e-01 +- nan
    [dimension 02/20]  active:      7.06e-01 +- 8.91e-03
    [dimension 03/20]  active:      3.53e-01 +- 7.13e-03
    [dimension 04/20]  inactive:    2.50e-04 +- 7.16e-03
    [dimension 05/20]  inactive:    -4.43e-04 +- 7.21e-03
    [dimension 06/20]  inactive:    -2.21e-04 +- 7.16e-03
    [dimension 07/20]  inactive:    1.39e-03 +- 7.37e-03
    [dimension 08/20]  inactive:    -9.10e-04 +- 7.23e-03
    [dimension 09/20]  inactive:    -4.50e-04 +- 7.18e-03
    [dimension 10/20]  inactive:    -2.45e-04 +- 7.19e-03
    [dimension 11/20]  inactive:    6.17e-04 +- 7.22e-03
    [dimension 12/20]  inactive:    -2.02e-03 +- 7.40e-03
    [dimension 13/20]  inactive:    7.08e-04 +- 7.22e-03
    [dimension 14/20]  inactive:    1.47e-04 +- 7.16e-03
    [dimension 15/20]  inactive:    1.60e-04 +- 7.15e-03
    [dimension 16/20]  inactive:    3.66e-04 +- 7.22e-03
    [dimension 17/20]  inactive:    -2.46e-03 +- 7.57e-03
    [dimension 18/20]  inactive:    -4.68e-05 +- 7.17e-03
    [dimension 19/20]  inactive:    3.24e-04 +- 7.16e-03
    [dimension 20/20]  inactive:    5.83e-04 +- 7.22e-03
    Identified a total of 3 active dimensions; expected 3.
    Identified pairwise interaction between dimensions 1 and 2: 4.56e-01 +- 6.34e-03
    Single posterior sample theta:
     [ 6.6538459e-01  6.9174665e-01  3.6366379e-01 -1.5317559e-03
      1.1384111e-02 -1.5346286e-03  2.7325670e-03 -8.9224270e-03
     -2.9880821e-03  2.8083615e-03  1.6365863e-02 -5.7469024e-03
      4.1483231e-03 -4.9307127e-03 -1.4497893e-02 -2.5221149e-03
     -2.7849204e-03 -6.9763307e-03 -4.8586656e-03  2.0306443e-03
      4.6054012e-01 -3.0863099e-05 -1.3017485e-02]






|


.. code-block:: default


    import argparse
    import itertools
    import os
    import time

    import numpy as np

    import jax
    from jax import vmap
    import jax.numpy as jnp
    import jax.random as random
    from jax.scipy.linalg import cho_factor, cho_solve, solve_triangular

    import numpyro
    import numpyro.distributions as dist
    from numpyro.infer import MCMC, NUTS


    def dot(X, Z):
        return jnp.dot(X, Z[..., None])[..., 0]


    # The kernel that corresponds to our quadratic regressor.
    def kernel(X, Z, eta1, eta2, c, jitter=1.0e-4):
        eta1sq = jnp.square(eta1)
        eta2sq = jnp.square(eta2)
        k1 = 0.5 * eta2sq * jnp.square(1.0 + dot(X, Z))
        k2 = -0.5 * eta2sq * dot(jnp.square(X), jnp.square(Z))
        k3 = (eta1sq - eta2sq) * dot(X, Z)
        k4 = jnp.square(c) - 0.5 * eta2sq
        if X.shape == Z.shape:
            k4 += jitter * jnp.eye(X.shape[0])
        return k1 + k2 + k3 + k4


    # Most of the model code is concerned with constructing the sparsity inducing prior.
    def model(X, Y, hypers):
        S, P, N = hypers['expected_sparsity'], X.shape[1], X.shape[0]

        sigma = numpyro.sample("sigma", dist.HalfNormal(hypers['alpha3']))
        phi = sigma * (S / jnp.sqrt(N)) / (P - S)
        eta1 = numpyro.sample("eta1", dist.HalfCauchy(phi))

        msq = numpyro.sample("msq", dist.InverseGamma(hypers['alpha1'], hypers['beta1']))
        xisq = numpyro.sample("xisq", dist.InverseGamma(hypers['alpha2'], hypers['beta2']))

        eta2 = jnp.square(eta1) * jnp.sqrt(xisq) / msq

        lam = numpyro.sample("lambda", dist.HalfCauchy(jnp.ones(P)))
        kappa = jnp.sqrt(msq) * lam / jnp.sqrt(msq + jnp.square(eta1 * lam))

        # compute kernel
        kX = kappa * X
        k = kernel(kX, kX, eta1, eta2, hypers['c']) + sigma ** 2 * jnp.eye(N)
        assert k.shape == (N, N)

        # sample Y according to the standard gaussian process formula
        numpyro.sample("Y", dist.MultivariateNormal(loc=jnp.zeros(X.shape[0]), covariance_matrix=k),
                       obs=Y)


    # Compute the mean and variance of coefficient theta_i (where i = dimension) for a
    # MCMC sample of the kernel hyperparameters (eta1, xisq, ...).
    # Compare to theorem 5.1 in reference [1].
    def compute_singleton_mean_variance(X, Y, dimension, msq, lam, eta1, xisq, c, sigma):
        P, N = X.shape[1], X.shape[0]

        probe = jnp.zeros((2, P))
        probe = jax.ops.index_update(probe, jax.ops.index[:, dimension], jnp.array([1.0, -1.0]))

        eta2 = jnp.square(eta1) * jnp.sqrt(xisq) / msq
        kappa = jnp.sqrt(msq) * lam / jnp.sqrt(msq + jnp.square(eta1 * lam))

        kX = kappa * X
        kprobe = kappa * probe

        k_xx = kernel(kX, kX, eta1, eta2, c) + sigma ** 2 * jnp.eye(N)
        k_xx_inv = jnp.linalg.inv(k_xx)
        k_probeX = kernel(kprobe, kX, eta1, eta2, c)
        k_prbprb = kernel(kprobe, kprobe, eta1, eta2, c)

        vec = jnp.array([0.50, -0.50])
        mu = jnp.matmul(k_probeX, jnp.matmul(k_xx_inv, Y))
        mu = jnp.dot(mu, vec)

        var = k_prbprb - jnp.matmul(k_probeX, jnp.matmul(k_xx_inv, jnp.transpose(k_probeX)))
        var = jnp.matmul(var, vec)
        var = jnp.dot(var, vec)

        return mu, var


    # Compute the mean and variance of coefficient theta_ij for a MCMC sample of the
    # kernel hyperparameters (eta1, xisq, ...). Compare to theorem 5.1 in reference [1].
    def compute_pairwise_mean_variance(X, Y, dim1, dim2, msq, lam, eta1, xisq, c, sigma):
        P, N = X.shape[1], X.shape[0]

        probe = jnp.zeros((4, P))
        probe = jax.ops.index_update(probe, jax.ops.index[:, dim1], jnp.array([1.0, 1.0, -1.0, -1.0]))
        probe = jax.ops.index_update(probe, jax.ops.index[:, dim2], jnp.array([1.0, -1.0, 1.0, -1.0]))

        eta2 = jnp.square(eta1) * jnp.sqrt(xisq) / msq
        kappa = jnp.sqrt(msq) * lam / jnp.sqrt(msq + jnp.square(eta1 * lam))

        kX = kappa * X
        kprobe = kappa * probe

        k_xx = kernel(kX, kX, eta1, eta2, c) + sigma ** 2 * jnp.eye(N)
        k_xx_inv = jnp.linalg.inv(k_xx)
        k_probeX = kernel(kprobe, kX, eta1, eta2, c)
        k_prbprb = kernel(kprobe, kprobe, eta1, eta2, c)

        vec = jnp.array([0.25, -0.25, -0.25, 0.25])
        mu = jnp.matmul(k_probeX, jnp.matmul(k_xx_inv, Y))
        mu = jnp.dot(mu, vec)

        var = k_prbprb - jnp.matmul(k_probeX, jnp.matmul(k_xx_inv, jnp.transpose(k_probeX)))
        var = jnp.matmul(var, vec)
        var = jnp.dot(var, vec)

        return mu, var


    # Sample coefficients theta from the posterior for a given MCMC sample.
    # The first P returned values are {theta_1, theta_2, ...., theta_P}, while
    # the remaining values are {theta_ij} for i,j in the list `active_dims`,
    # sorted so that i < j.
    def sample_theta_space(X, Y, active_dims, msq, lam, eta1, xisq, c, sigma):
        P, N, M = X.shape[1], X.shape[0], len(active_dims)
        # the total number of coefficients we return
        num_coefficients = P + M * (M - 1) // 2

        probe = jnp.zeros((2 * P + 2 * M * (M - 1), P))
        vec = jnp.zeros((num_coefficients, 2 * P + 2 * M * (M - 1)))
        start1 = 0
        start2 = 0

        for dim in range(P):
            probe = jax.ops.index_update(probe, jax.ops.index[start1:start1 + 2, dim], jnp.array([1.0, -1.0]))
            vec = jax.ops.index_update(vec, jax.ops.index[start2, start1:start1 + 2], jnp.array([0.5, -0.5]))
            start1 += 2
            start2 += 1

        for dim1 in active_dims:
            for dim2 in active_dims:
                if dim1 >= dim2:
                    continue
                probe = jax.ops.index_update(probe, jax.ops.index[start1:start1 + 4, dim1],
                                             jnp.array([1.0, 1.0, -1.0, -1.0]))
                probe = jax.ops.index_update(probe, jax.ops.index[start1:start1 + 4, dim2],
                                             jnp.array([1.0, -1.0, 1.0, -1.0]))
                vec = jax.ops.index_update(vec, jax.ops.index[start2, start1:start1 + 4],
                                           jnp.array([0.25, -0.25, -0.25, 0.25]))
                start1 += 4
                start2 += 1

        eta2 = jnp.square(eta1) * jnp.sqrt(xisq) / msq
        kappa = jnp.sqrt(msq) * lam / jnp.sqrt(msq + jnp.square(eta1 * lam))

        kX = kappa * X
        kprobe = kappa * probe

        k_xx = kernel(kX, kX, eta1, eta2, c) + sigma ** 2 * jnp.eye(N)
        L = cho_factor(k_xx, lower=True)[0]
        k_probeX = kernel(kprobe, kX, eta1, eta2, c)
        k_prbprb = kernel(kprobe, kprobe, eta1, eta2, c)

        mu = jnp.matmul(k_probeX, cho_solve((L, True), Y))
        mu = jnp.sum(mu * vec, axis=-1)

        Linv_k_probeX = solve_triangular(L, jnp.transpose(k_probeX), lower=True)
        covar = k_prbprb - jnp.matmul(jnp.transpose(Linv_k_probeX), Linv_k_probeX)
        covar = jnp.matmul(vec, jnp.matmul(covar, jnp.transpose(vec)))

        # sample from N(mu, covar)
        L = jnp.linalg.cholesky(covar)
        sample = mu + jnp.matmul(L, np.random.randn(num_coefficients))

        return sample


    # Helper function for doing HMC inference
    def run_inference(model, args, rng_key, X, Y, hypers):
        start = time.time()
        kernel = NUTS(model)
        mcmc = MCMC(kernel, args.num_warmup, args.num_samples, num_chains=args.num_chains,
                    progress_bar=False if "NUMPYRO_SPHINXBUILD" in os.environ else True)
        mcmc.run(rng_key, X, Y, hypers)
        mcmc.print_summary()
        print('\nMCMC elapsed time:', time.time() - start)
        return mcmc.get_samples()


    # Get the mean and variance of a gaussian mixture
    def gaussian_mixture_stats(mus, variances):
        mean_mu = jnp.mean(mus)
        mean_var = jnp.mean(variances) + jnp.mean(jnp.square(mus)) - jnp.square(mean_mu)
        return mean_mu, mean_var


    # Create artificial regression dataset where only S out of P feature
    # dimensions contain signal and where there is a single pairwise interaction
    # between the first and second dimensions.
    def get_data(N=20, S=2, P=10, sigma_obs=0.05):
        assert S < P and P > 1 and S > 0
        np.random.seed(0)

        X = np.random.randn(N, P)
        # generate S coefficients with non-negligible magnitude
        W = 0.5 + 2.5 * np.random.rand(S)
        # generate data using the S coefficients and a single pairwise interaction
        Y = np.sum(X[:, 0:S] * W, axis=-1) + X[:, 0] * X[:, 1] + sigma_obs * np.random.randn(N)
        Y -= jnp.mean(Y)
        Y_std = jnp.std(Y)

        assert X.shape == (N, P)
        assert Y.shape == (N,)

        return X, Y / Y_std, W / Y_std, 1.0 / Y_std


    # Helper function for analyzing the posterior statistics for coefficient theta_i
    def analyze_dimension(samples, X, Y, dimension, hypers):
        vmap_args = (samples['msq'], samples['lambda'], samples['eta1'], samples['xisq'], samples['sigma'])
        mus, variances = vmap(lambda msq, lam, eta1, xisq, sigma:
                              compute_singleton_mean_variance(X, Y, dimension, msq, lam,
                                                              eta1, xisq, hypers['c'], sigma))(*vmap_args)
        mean, variance = gaussian_mixture_stats(mus, variances)
        std = jnp.sqrt(variance)
        return mean, std


    # Helper function for analyzing the posterior statistics for coefficient theta_ij
    def analyze_pair_of_dimensions(samples, X, Y, dim1, dim2, hypers):
        vmap_args = (samples['msq'], samples['lambda'], samples['eta1'], samples['xisq'], samples['sigma'])
        mus, variances = vmap(lambda msq, lam, eta1, xisq, sigma:
                              compute_pairwise_mean_variance(X, Y, dim1, dim2, msq, lam,
                                                             eta1, xisq, hypers['c'], sigma))(*vmap_args)
        mean, variance = gaussian_mixture_stats(mus, variances)
        std = jnp.sqrt(variance)
        return mean, std


    def main(args):
        X, Y, expected_thetas, expected_pairwise = get_data(N=args.num_data, P=args.num_dimensions,
                                                            S=args.active_dimensions)

        # setup hyperparameters
        hypers = {'expected_sparsity': max(1.0, args.num_dimensions / 10),
                  'alpha1': 3.0, 'beta1': 1.0,
                  'alpha2': 3.0, 'beta2': 1.0,
                  'alpha3': 1.0, 'c': 1.0}

        # do inference
        rng_key = random.PRNGKey(0)
        samples = run_inference(model, args, rng_key, X, Y, hypers)

        # compute the mean and square root variance of each coefficient theta_i
        means, stds = vmap(lambda dim: analyze_dimension(samples, X, Y, dim, hypers))(jnp.arange(args.num_dimensions))

        print("Coefficients theta_1 to theta_%d used to generate the data:" % args.active_dimensions, expected_thetas)
        print("The single quadratic coefficient theta_{1,2} used to generate the data:", expected_pairwise)
        active_dimensions = []

        for dim, (mean, std) in enumerate(zip(means, stds)):
            # we mark the dimension as inactive if the interval [mean - 3 * std, mean + 3 * std] contains zero
            lower, upper = mean - 3.0 * std, mean + 3.0 * std
            inactive = "inactive" if lower < 0.0 and upper > 0.0 else "active"
            if inactive == "active":
                active_dimensions.append(dim)
            print("[dimension %02d/%02d]  %s:\t%.2e +- %.2e" % (dim + 1, args.num_dimensions, inactive, mean, std))

        print("Identified a total of %d active dimensions; expected %d." % (len(active_dimensions),
                                                                            args.active_dimensions))

        # Compute the mean and square root variance of coefficients theta_ij for i,j active dimensions.
        # Note that the resulting numbers are only meaningful for i != j.
        if len(active_dimensions) > 0:
            dim_pairs = jnp.array(list(itertools.product(active_dimensions, active_dimensions)))
            means, stds = vmap(lambda dim_pair: analyze_pair_of_dimensions(samples, X, Y,
                                                                           dim_pair[0], dim_pair[1], hypers))(dim_pairs)
            for dim_pair, mean, std in zip(dim_pairs, means, stds):
                dim1, dim2 = dim_pair
                if dim1 >= dim2:
                    continue
                lower, upper = mean - 3.0 * std, mean + 3.0 * std
                if not (lower < 0.0 and upper > 0.0):
                    format_str = "Identified pairwise interaction between dimensions %d and %d: %.2e +- %.2e"
                    print(format_str % (dim1 + 1, dim2 + 1, mean, std))

            # Draw a single sample of coefficients theta from the posterior, where we return all singleton
            # coefficients theta_i and pairwise coefficients theta_ij for i, j active dimensions. We use the
            # final MCMC sample obtained from the HMC sampler.
            thetas = sample_theta_space(X, Y, active_dimensions, samples['msq'][-1], samples['lambda'][-1],
                                        samples['eta1'][-1], samples['xisq'][-1], hypers['c'], samples['sigma'][-1])
            print("Single posterior sample theta:\n", thetas)


    if __name__ == "__main__":
        assert numpyro.__version__.startswith('0.4.0')
        parser = argparse.ArgumentParser(description="Gaussian Process example")
        parser.add_argument("-n", "--num-samples", nargs="?", default=1000, type=int)
        parser.add_argument("--num-warmup", nargs='?', default=500, type=int)
        parser.add_argument("--num-chains", nargs='?', default=1, type=int)
        parser.add_argument("--num-data", nargs='?', default=100, type=int)
        parser.add_argument("--num-dimensions", nargs='?', default=20, type=int)
        parser.add_argument("--active-dimensions", nargs='?', default=3, type=int)
        parser.add_argument("--device", default='cpu', type=str, help='use "cpu" or "gpu".')
        args = parser.parse_args()

        numpyro.set_platform(args.device)
        numpyro.set_host_device_count(args.num_chains)

        main(args)


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  38.528 seconds)


.. _sphx_glr_download_examples_sparse_regression.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: sparse_regression.py <sparse_regression.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: sparse_regression.ipynb <sparse_regression.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
